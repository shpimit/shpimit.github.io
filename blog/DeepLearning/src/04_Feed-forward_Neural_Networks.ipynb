{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Feed-forward_Neural_Networks\n",
    "In this notebook, we will see how to define simple feed-foward neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f873d1ef90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the networkâ€™s parameters\n",
    "- Update the weights of the network, typically using an optimizer.\n",
    "\n",
    "We will look at all the above processes with a concrete example, MNIST.\n",
    "\n",
    "### Define the network\n",
    "First of all, we need a new feed-foward neural network for performing image classification on MNIST.\n",
    "In PyTorch, you can build your own neural network using the `torch.nn`package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 784\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Define the operations to use for input processing.\n",
    "        # torch.nn.Linear(in_features, out_features, bias=True)\n",
    "        #               : a linear projection(fc) layer(in_feeatures -> out_features)\n",
    "        # torch.nn.RELU(inplace=False): a ReLU activation function\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the input processing through network\n",
    "        z1 = self.fc1(x)\n",
    "        h1 = self.relu(z1)\n",
    "        out = self.fc2(h1)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the `forward` function, and the `backward` function (where gradients are computed) is automatically defined for you using `autograd`.\n",
    "\n",
    "The architecture of the above `NeuralNet` is as follows:\n",
    "<img src=\"images/nn_architecture.png\" width=\"500\">\n",
    "\n",
    "Here, x and y are the input, target (true label) values, respectively.\n",
    "\n",
    "The learnable parameters of a model are returned by `model.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([256, 784])\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # fc1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and Optimizer\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different loss functions under the nn package.\n",
    "We use `nn.CrossEntropyLoss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0980, -0.5035,  0.0184,  0.0011,  0.0577,  0.2312,  0.1687,  0.0417,\n",
      "         -0.1924, -0.0033]], grad_fn=<ThAddmmBackward>)\n",
      "tensor(2.3904, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 784) # a random input, for example\n",
    "output = model(input) # output: (batch_size, num_classes)\n",
    "print(output)\n",
    "\n",
    "target = torch.tensor([0])  # a dummy target, for example. target: (batch_size) where 0 <= each element < num_classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furtheremore, PyTorch supports several optimizers from `torch.optim`.\n",
    "We use an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f876956470>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# plot one example\n",
    "print(train_dataset.train_data.size())                 # (60000, 28, 28)\n",
    "print(train_dataset.train_labels.size())               # (60000)\n",
    "\n",
    "idx = 0\n",
    "plt.title('%d' % train_dataset.train_labels[idx].item())\n",
    "plt.imshow(train_dataset.train_data[idx,:,:].numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.7585\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3331\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2676\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2476\n",
      "Epoch [1/5], Step [500/600], Loss: 0.2130\n",
      "Epoch [1/5], Step [600/600], Loss: 0.2064\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1720\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1571\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1458\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1467\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1403\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1157\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1057\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1060\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1023\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0955\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0909\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0918\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0748\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0810\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0664\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0731\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0700\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0737\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0523\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0524\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0582\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0567\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0541\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0557\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), './data/nn_model.ckpt')\n",
    "\n",
    "# Load the model checkpoint if needed\n",
    "# new_model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "# new_model.load_state_dict(torch.load('./data/nn_model.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: CIFAR10\n",
    "\n",
    "<img src=\"images/cifar10.png\" width=\"400\">\n",
    "\n",
    "The CIFAR-10 dataset has the following specification:\n",
    "- The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "- CIFAR-10 has the ten classes: â€˜airplaneâ€™, â€˜automobileâ€™, â€˜birdâ€™, â€˜catâ€™, â€˜deerâ€™, â€˜dogâ€™, â€˜frogâ€™, â€˜horseâ€™, â€˜shipâ€™, â€˜truckâ€™.\n",
    "\n",
    "You have to define a feed-foward neural network with two hidden layers for performing image classifcation on the CIFAR-10 dataset as well as train and test the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 3*32*32\n",
    "hidden1_size = 512\n",
    "hidden2_size = 128\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# transform images to tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Write the code to define a neural network with two hidden layers.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        \n",
    "        # ========================================\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        return x # DUMMY\n",
    "        # ========================================\n",
    "\n",
    "model = Net(input_size, hidden1_size, hidden2_size, num_classes).to(device)\n",
    "print(model)\n",
    "\n",
    "# Write the code to train and test the network.\n",
    "# ============ YOUR CODE HERE ============\n",
    "        \n",
    "# ========================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
