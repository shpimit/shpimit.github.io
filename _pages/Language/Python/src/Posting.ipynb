{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    (\"Document1\", \"This is a sample\"),\n",
    "    (\"Document2\", \"This is another sample\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1\n",
      "Document2\n"
     ]
    }
   ],
   "source": [
    "# in-memory\n",
    "# 전체 색인어 목록(Dictionary)\n",
    "# {단어1:포스팅위치-fptr, 단어2:포스팅위치, ...} fseek(fptr), fread(4byters*4)\n",
    "globalLexicon = dict()\n",
    "# 전체 문서 목롤(Dictionary)\n",
    "# [0:문서1-이름, 1:문서2-이름, ...]\n",
    "globalDocument = list()\n",
    "# disk\n",
    "# 사전에 있는 색인어 중, 어는 문서에서, 몇번 나탔는지\n",
    "# Tuple(lexiconIdx, documentIdx, freq, 다음포스틩위치-ptr)\n",
    "globalPosting = list()\n",
    "\n",
    "for (docName, docContent) in collection:\n",
    "#     0: Document1\n",
    "#         len = 1\n",
    "#     1: document2\n",
    "    docIdx = len(globalDocument) # Crawling - Duplicated X\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # 문서 1개에 대해서만,\n",
    "    # {색인어1의 idx:빈도, 색인어2:빈도, ...}\n",
    "    localPosting = dict()\n",
    "    \n",
    "    for token in docContent.lower().split():\n",
    "        # Local에 대해서, 없으면 추가\n",
    "        if token not in localPosting.keys():\n",
    "            localPosting[token] =1\n",
    "        # 있으면, 빈도 증가\n",
    "        else:\n",
    "            localPosting[token] += 1\n",
    "            \n",
    "    # Global Merge\n",
    "    for token, freq in localPosting.items(): \n",
    "\n",
    "        if token not in globalLexicon.keys(): #         if 색인어 목록 X -> 새로운 단어 추가\n",
    "            \n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting)\n",
    "            postingData = (lexiconIdx, docIdx, freq, -1) # -1은 마지막 위치라는 것을 가리키기 위해\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx\n",
    "            # 글로발(사전)에서 위치만 가져온, 나중에 posting 위치를 추가\n",
    "        else: #         else 기존단어의 idx 가져오기\n",
    "            lexiconIdx = list(globalLexicon.keys()).index(token)\n",
    "            postingIdx = len(globalPosting)\n",
    "            oldPostingIdx = globalLexicon[token]\n",
    "            postingData = (lexiconIdx, docIdx, freq, oldPostingIdx) \n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx\n",
    "\n",
    "    print(docName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 5, 'a': 2, 'sample': 7, 'another': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 1, -1),\n",
       " (1, 0, 1, -1),\n",
       " (2, 0, 1, -1),\n",
       " (3, 0, 1, -1),\n",
       " (0, 1, 1, 0),\n",
       " (1, 1, 1, 1),\n",
       " (4, 1, 1, -1),\n",
       " (3, 1, 1, 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalLexicon['sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 1, -1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalPosting[globalLexicon['sample']][3]]   # 다음 주소가 \"-1\" 일때 까지 반복해서 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    ('Document1', 'This is a a a a a a a a a a sample'),\n",
    "    ('Document2', 'This is a sample'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      " DocName:Document2 - TermFreq:1 - Next:0 \n",
      " DocName:Document1 - TermFreq:1 - Next:-1 \n",
      "\n",
      "is\n",
      " DocName:Document2 - TermFreq:1 - Next:1 \n",
      " DocName:Document1 - TermFreq:1 - Next:-1 \n",
      "\n",
      "a\n",
      " DocName:Document1 - TermFreq:1 - Next:-1 \n",
      "\n",
      "sample\n",
      " DocName:Document2 - TermFreq:1 - Next:3 \n",
      " DocName:Document1 - TermFreq:1 - Next:-1 \n",
      "\n",
      "another\n",
      " DocName:Document2 - TermFreq:1 - Next:-1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# indexTerm\n",
    "for indexTerm, postingIdx in globalLexicon.items():\n",
    "    print(indexTerm)\n",
    "    \n",
    "    while True:\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        print(\" DocName:{0} - TermFreq:{1} - Next:{2} \".format(\n",
    "             globalDocument[postingData[1]],\n",
    "             postingData[2],\n",
    "             postingData[3]\n",
    "        ))\n",
    "        \n",
    "        postingIdx = postingData[3]\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10, log2\n",
    "# TF\n",
    "def binaryTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def rawTF(freq):\n",
    "    return freq\n",
    "\n",
    "def basicTF(freq, totalFreq):\n",
    "    return freq/totalFreq\n",
    "\n",
    "def logTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1 + log10(freq)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def doubleNormalTF(K, freq, maxFreq):\n",
    "    return K + ((1-K) * (freq/ maxFreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "1. Binary: 1\n",
      "2. Raw : 1\n",
      "3. Basic : 0.07692307692307693\n",
      "4. Log : 1.0\n",
      "5. DoubleNormalization : 0.1\n",
      "6. DoubleNormalization : 0.55\n",
      "\n",
      "is\n",
      "1. Binary: 1\n",
      "2. Raw : 1\n",
      "3. Basic : 0.07692307692307693\n",
      "4. Log : 1.0\n",
      "5. DoubleNormalization : 0.1\n",
      "6. DoubleNormalization : 0.55\n",
      "\n",
      "a\n",
      "1. Binary: 1\n",
      "2. Raw : 10\n",
      "3. Basic : 0.7692307692307693\n",
      "4. Log : 2.0\n",
      "5. DoubleNormalization : 1.0\n",
      "6. DoubleNormalization : 1.0\n",
      "\n",
      "sample\n",
      "1. Binary: 1\n",
      "2. Raw : 1\n",
      "3. Basic : 0.07692307692307693\n",
      "4. Log : 1.0\n",
      "5. DoubleNormalization : 0.1\n",
      "6. DoubleNormalization : 0.55\n",
      "\n",
      "this\n",
      "1. Binary: 1\n",
      "2. Raw : 1\n",
      "3. Basic : 0.25\n",
      "4. Log : 1.0\n",
      "5. DoubleNormalization : 1.0\n",
      "6. DoubleNormalization : 1.0\n",
      "\n",
      "is\n",
      "1. Binary: 1\n",
      "2. Raw : 1\n",
      "3. Basic : 0.25\n",
      "4. Log : 1.0\n",
      "5. DoubleNormalization : 1.0\n",
      "6. DoubleNormalization : 1.0\n",
      "\n",
      "a\n",
      "1. Binary: 1\n",
      "2. Raw : 1\n",
      "3. Basic : 0.25\n",
      "4. Log : 1.0\n",
      "5. DoubleNormalization : 1.0\n",
      "6. DoubleNormalization : 1.0\n",
      "\n",
      "sample\n",
      "1. Binary: 1\n",
      "2. Raw : 1\n",
      "3. Basic : 0.25\n",
      "4. Log : 1.0\n",
      "5. DoubleNormalization : 1.0\n",
      "6. DoubleNormalization : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (docName, docContent) in collection:\n",
    "\n",
    "    localPosting = dict()\n",
    "    \n",
    "    for term in docContent.lower().split():\n",
    "        # Local에 대해서, 없으면 추가\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] =1\n",
    "        # 있으면, 빈도 증가\n",
    "        else:\n",
    "            localPosting[term] +=1\n",
    "            \n",
    "    maxFreq = max(localPosting.values())\n",
    "    totalCount = sum(localPosting.values())\n",
    "    \n",
    "    for token, freq in localPosting.items():\n",
    "        print(token)   \n",
    "        print(\"1. Binary: {0}\".format(binaryTF(freq)))\n",
    "        print(\"2. Raw : {0}\".format(rawTF(freq)))\n",
    "        print(\"3. Basic : {0}\".format(basicTF(freq,totalCount)))\n",
    "        print(\"4. Log : {0}\".format(logTF(freq)))\n",
    "        print(\"5. DoubleNormalization : {0}\".format(doubleNormalTF(0,freq, maxFreq )))   #\n",
    "        print(\"6. DoubleNormalization : {0}\".format(doubleNormalTF(0.5,freq, maxFreq )))\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unaryIDF():\n",
    "    return 1\n",
    "\n",
    "def basicIDF(N, df):\n",
    "    return log10(N/df)\n",
    "\n",
    "def smoothingIDF(N, df):\n",
    "    return log10((1+N)/df)  #  < 0 (N=df) 이 경우 -값이 나온다\n",
    "\n",
    "def probabilityIDF(N, df):\n",
    "    return log10((N-df+1)/df) #  절반보단 많이 나오면 - 절반보다 적개나오면...+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 5, 'a': 2, 'sample': 7, 'another': 6}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalLexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "1. UnaryIDF: 1\n",
      "2. BasicIDF : 0.0\n",
      "3. SmoothingIDF : 0.17609125905568124\n",
      "4. ProbabilityIDF : -0.3010299956639812\n",
      "is\n",
      "1. UnaryIDF: 1\n",
      "2. BasicIDF : 0.0\n",
      "3. SmoothingIDF : 0.17609125905568124\n",
      "4. ProbabilityIDF : -0.3010299956639812\n",
      "a\n",
      "1. UnaryIDF: 1\n",
      "2. BasicIDF : 0.3010299956639812\n",
      "3. SmoothingIDF : 0.47712125471966244\n",
      "4. ProbabilityIDF : 0.3010299956639812\n",
      "sample\n",
      "1. UnaryIDF: 1\n",
      "2. BasicIDF : 0.0\n",
      "3. SmoothingIDF : 0.17609125905568124\n",
      "4. ProbabilityIDF : -0.3010299956639812\n",
      "another\n",
      "1. UnaryIDF: 1\n",
      "2. BasicIDF : 0.3010299956639812\n",
      "3. SmoothingIDF : 0.47712125471966244\n",
      "4. ProbabilityIDF : 0.3010299956639812\n"
     ]
    }
   ],
   "source": [
    "N = len(globalDocument)\n",
    "\n",
    "for term, ptr in globalLexicon.items():\n",
    "    \n",
    "    df = 0\n",
    "    \n",
    "    while True:\n",
    "        if ptr == -1:\n",
    "            break\n",
    "            \n",
    "        df += 1\n",
    "        \n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "    print(term)\n",
    "#     print(\"{0}. {1}\".format(term, df))\n",
    "    print(\"1. UnaryIDF: {0}\".format(unaryIDF()))\n",
    "    print(\"2. BasicIDF : {0}\".format(basicIDF(N, df)))\n",
    "    print(\"3. SmoothingIDF : {0}\".format(smoothingIDF(N,df)))\n",
    "    print(\"4. ProbabilityIDF : {0}\".format(probabilityIDF(N, df)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
