{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'd\", 'like', 'to', 'learn', 'more', 'something.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I'd like to learn more something.\"\n",
    "\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'d\", 'like', 'to', 'learn', 'more', 'something', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to learn more something.\n",
      "Id like to learn more something\n"
     ]
    }
   ],
   "source": [
    "## 고빈도 단어 . 을 없앤다.\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r\"[{0}]\".format(re.escape(punctuation)))\n",
    "print(sentence)\n",
    "print(pattern.sub(\"\",sentence))\n",
    "# 축약어, 신조어 조심    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'd', 'like', 'to', 'learn', 'more', 'something']\n"
     ]
    }
   ],
   "source": [
    "result= []\n",
    "for term in word_tokenize(sentence):\n",
    "    new = pattern.sub(\"\", term)\n",
    "    \n",
    "    if new:\n",
    "        result.append(new)\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"파이썬, 자연어처리, 화요일, 힘듦!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬, 자연어처리, 화요일, 힘듦!\n",
      "파이썬 자연어처리 화요일 힘듦\n",
      "['파이썬', '자연어처리', '화요일', '힘듦']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(pattern.sub(\"\",sentence))\n",
    "\n",
    "result= []\n",
    "for term in word_tokenize(sentence):\n",
    "    new = pattern.sub(\"\", term)\n",
    "    \n",
    "    if new:\n",
    "        result.append(new)\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords(불용어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shpim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,\n",
       " ['arabic',\n",
       "  'azerbaijani',\n",
       "  'danish',\n",
       "  'dutch',\n",
       "  'english',\n",
       "  'finnish',\n",
       "  'french',\n",
       "  'german',\n",
       "  'greek',\n",
       "  'hungarian',\n",
       "  'indonesian',\n",
       "  'italian',\n",
       "  'kazakh',\n",
       "  'nepali',\n",
       "  'norwegian',\n",
       "  'portuguese',\n",
       "  'romanian',\n",
       "  'russian',\n",
       "  'spanish',\n",
       "  'swedish',\n",
       "  'turkish'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "len(stopwords.fileids()), stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped\n",
      "love\n",
      "Skipped\n",
      "['love']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.open(\"english\").read()\n",
    "\n",
    "sentence = \"i love you\"\n",
    "sentence.split(), word_tokenize(sentence)\n",
    "\n",
    "result=[]\n",
    "\n",
    "for token in word_tokenize(sentence):\n",
    "    if token in stop:\n",
    "        print(\"Skipped\")\n",
    "    else:\n",
    "        print(token)\n",
    "        result.append(token)\n",
    "        \n",
    "print(result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"i'd like to learn more something.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped\n",
      "Skipped\n",
      "like\n",
      "Skipped\n",
      "learn\n",
      "Skipped\n",
      "something\n",
      ".\n",
      "['like', 'learn', 'something', '.']\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "\n",
    "for token in word_tokenize(sentence):\n",
    "    if token in stop:\n",
    "        print(\"Skipped\")\n",
    "    else:\n",
    "        print(token)\n",
    "        result.append(token)\n",
    "        \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = [\"은\", \"는\", \"이\",\"가\",\"을\",\"를\",\"에게\", \"께서\"]\n",
    "\n",
    "sentence = \"어머님 은 짜장면 이 싫다 고 하셨 어\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어머님\n",
      "Skipped\n",
      "짜장면\n",
      "Skipped\n",
      "['어머님', '짜장면']\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "\n",
    "threshold =2\n",
    "\n",
    "for token in word_tokenize(sentence):\n",
    "    if token in stop:\n",
    "        print(\"Skipped\")\n",
    "    else:\n",
    "        if len(token) > threshold:\n",
    "            print(token)\n",
    "            result.append(token)\n",
    "        \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FreqDist' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-63a12c29524c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtextObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#         if len(token) > threshold:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FreqDist' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "from nltk import Text\n",
    "\n",
    "textObj = Text(word_tokenize(sentence)).vocab()\n",
    "    \n",
    "result=[]\n",
    "\n",
    "threshold =2\n",
    "pattern = re.compile(\"\\b\\w{1,%d}\\b\" % threshold)\n",
    "\n",
    "stop = [\"은\", \"는\", \"이\",\"가\",\"을\",\"를\",\"에게\", \"께서\"]\n",
    "\n",
    "sentence = \"어머님 은 짜장면 이 싫다 고 하셨 어, 어머님 은 짜장면 이 싫다\"\n",
    "sentence = re.sub(\"[{0}]\".format(punctuation), \"\", sentence)\n",
    "sentence.split(), word_tokenize(sentence)\n",
    "\n",
    "for token in textObj:\n",
    "    if token in stop:\n",
    "        print(\"Skipped\")\n",
    "    else:\n",
    "        new = pattern.sub(\"\", token)\n",
    "        if new and textObj.count(token) > threshold:\n",
    "#         if len(token) > threshold:\n",
    "            print(new)\n",
    "            result.append(new)\n",
    "            \n",
    "print(result)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 불용어와 비속어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시발짜증나네 **\n"
     ]
    }
   ],
   "source": [
    "stop = [\"시발\", \"씨발\"]\n",
    "\n",
    "sentence = \"시발짜증나네 씨발\"\n",
    "result = []\n",
    "\n",
    "# split 띄어쓰기 단위  : 어절\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "#         print(\"Skipped\")\n",
    "        result.append(\"*\"*len(term))\n",
    "    else:\n",
    "#         print(term)\n",
    "        result.append(term)\n",
    "        \n",
    "print(\" \".join(result))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramUmjeol(term, n=2):\n",
    "    '''\n",
    "    입력 :    음절1,   음절2,   음절3,  음절4 :  4\n",
    "    출력(2) : 음절12,  음절23,  음절34 :         3 - n + 1\n",
    "    출력(3) : 음절123, 음절234         :         2 - n + 1\n",
    "    '''\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(term) - n + 1):\n",
    "        ngram.append(''.join(term[i:i + n]))   \n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** **\n"
     ]
    }
   ],
   "source": [
    "stop = [\"시발\", \"씨발\"]\n",
    "\n",
    "sentence = \"시발짜증나네 씨발\"\n",
    "result = []\n",
    "\n",
    "# split 띄어쓰기 단위  : 어절\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "#         print(\"Skipped\")\n",
    "        result.append(\"*\"*len(term))\n",
    "    else:\n",
    "        flag = True\n",
    "        \n",
    "        for ngram in ngramUmjeol(term):\n",
    "            if ngram in stop:\n",
    "                flag = False\n",
    "                break\n",
    "                print(\"Skipped\")\n",
    "        if flag:\n",
    "            result.append(term)\n",
    "        else:\n",
    "            result.append(\"*\"*len(term))\n",
    "#         print(term)\n",
    "#         result.append(term)\n",
    "        \n",
    "print(\" \".join(result))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규식을 이용함\n",
    "## 1. 한글만 뽑던지\n",
    "## 2. 단어라고 하는 영역 안에 있는 다른 글자 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 2 required positional arguments: 'repl' and 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-96cc2ec05702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"시123123발짜증나네 씨발 2씨발\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"[{0}]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sub() missing 2 required positional arguments: 'repl' and 'string'"
     ]
    }
   ],
   "source": [
    "stop = [\"시발\", \"씨발\"]\n",
    "\n",
    "sentence = \"시123123발짜증나네 씨발 2씨발\"\n",
    "pattern = re.sub(r\"[{0}]\".format(re.escape(punctuation)))\n",
    "result = []\n",
    "\n",
    "# split 띄어쓰기 단위  : 어절\n",
    "\n",
    "for term in sentence.split():\n",
    "    if term in stop:\n",
    "#         print(\"Skipped\")\n",
    "        result.append(\"*\"*len(term))\n",
    "    else:\n",
    "        flag = True\n",
    "        \n",
    "        print(\"> \"+term)\n",
    "        re.sub(r\"\\B[0-9]+\\B\", \"\", term)\n",
    "        print(term)\n",
    "        \n",
    "        for ngram in ngramUmjeol(term):\n",
    "            if ngram in stop:\n",
    "                flag = False\n",
    "                break\n",
    "                print(\"Skipped\")\n",
    "        if flag:\n",
    "            result.append(term)\n",
    "        else:\n",
    "            result.append(\"*\"*len(term))\n",
    "#         print(term)\n",
    "#         result.append(term)\n",
    "        \n",
    "print(\" \".join(result))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeNgram(maxKey, data):\n",
    "    result = defaultdict(int)\n",
    "    \n",
    "    token = ' '.join(maxKey)\n",
    "    pattern = re.compile(r'(?!=\\S)' + token + '(?!\\S)') # S : whitespace\n",
    "    \n",
    "    for k, v in data.items():\n",
    "        new = pattern.sub(''.join(maxKey), k)\n",
    "        result[new] = v\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTerm(sentence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5ac4145a14e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#리스트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "pattern = max(tokens, key=tokens.get)\n",
    "#리스트\n",
    "pattern[0], pattern[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte Per Encoding 패턴을 가지고   시작과 끝싸이에 어떤 값이 오더라도   다 찾아낸다.\n",
    "pattern = re.compile(r\"{0}.*{1}\".format(pattern[0], pattern[1]))\n",
    "pattern\n",
    "                     \n",
    "# Normalization\n",
    "# 구두점 제거,  글자수 적은것 제거, 영문 lower\n",
    "# bigram, regular expression, BYPER..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
