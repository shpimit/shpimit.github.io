{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10, log2\n",
    "# TF\n",
    "def binaryTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def rawTF(freq):\n",
    "    return freq\n",
    "\n",
    "def basicTF(freq, totalFreq):\n",
    "    return freq/totalFreq\n",
    "\n",
    "def logTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1 + log10(freq)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def doubleNormalTF(K, freq, maxFreq):\n",
    "    return K + ((1-K) * (freq/ maxFreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1\n",
      "Document2\n",
      "Document3\n",
      "Document4\n",
      "Document5\n",
      "Document6\n"
     ]
    }
   ],
   "source": [
    "collection = [\n",
    "    (\"Document1\", \"This is a sample\"),\n",
    "    (\"Document2\", \"This is another sample\"),\n",
    "    (\"Document3\", \"This is not sample\"),\n",
    "    (\"Document4\", \"a not sample\"),\n",
    "    (\"Document5\", \"not\"),\n",
    "    (\"Document6\", \"not sample\")\n",
    "]\n",
    "\n",
    "globalLexicon = dict()\n",
    "globalDocument = list()\n",
    "globalPosting = list()\n",
    "\n",
    "for (docName, docContent) in collection:\n",
    "    docIdx = len(globalDocument) # Crawling - Duplicated X\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    localPosting = dict()\n",
    "    \n",
    "    for token in docContent.lower().split():\n",
    "        # Local에 대해서, 없으면 추가\n",
    "        if token not in localPosting.keys():\n",
    "            localPosting[token] =1\n",
    "        # 있으면, 빈도 증가\n",
    "        else:\n",
    "            localPosting[token] += 1\n",
    "            \n",
    "    maxFreq = max(localPosting.values())\n",
    "            \n",
    "    # Global Merge\n",
    "    for token, freq in localPosting.items(): \n",
    "\n",
    "        if token not in globalLexicon.keys(): #         if 색인어 목록 X -> 새로운 단어 추가\n",
    "            \n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting)\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, freq, maxFreq), -1] # -1은 마지막 위치라는 것을 가리키기 위해\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx\n",
    "            # 글로발(사전)에서 위치만 가져온, 나중에 posting 위치를 추가\n",
    "        else: #         else 기존단어의 idx 가져오기\n",
    "            lexiconIdx = list(globalLexicon.keys()).index(token)\n",
    "            postingIdx = len(globalPosting)\n",
    "            oldPostingIdx = globalLexicon[token]\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, freq, maxFreq), oldPostingIdx]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx\n",
    "\n",
    "    print(docName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1.0, -1],\n",
       " [1, 0, 1.0, -1],\n",
       " [2, 0, 1.0, -1],\n",
       " [3, 0, 1.0, -1],\n",
       " [0, 1, 1.0, 0],\n",
       " [1, 1, 1.0, 1],\n",
       " [4, 1, 1.0, -1],\n",
       " [3, 1, 1.0, 3],\n",
       " [0, 2, 1.0, 4],\n",
       " [1, 2, 1.0, 5],\n",
       " [5, 2, 1.0, -1],\n",
       " [3, 2, 1.0, 7],\n",
       " [2, 3, 1.0, 2],\n",
       " [5, 3, 1.0, 10],\n",
       " [3, 3, 1.0, 11],\n",
       " [5, 4, 1.0, 13],\n",
       " [5, 5, 1.0, 15],\n",
       " [3, 5, 1.0, 14]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unaryIDF():\n",
    "    return 1\n",
    "\n",
    "def basicIDF(N, df):\n",
    "    return log10(N/df)\n",
    "\n",
    "def smoothingIDF(N, df):\n",
    "    return log10((1+N)/df)  #  < 0 (N=df) 이 경우 -값이 나온다\n",
    "\n",
    "def probabilityIDF(N, df):\n",
    "    return log10((N-df+1)/df) #  절반보단 많이 나오면 - 절반보다 적개나오면...+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "단어:this / IDF:0.3010299956639812\n",
      "문서:Document3 / TF:0.09061905828945654 / TF-IDF:0.02727905472394916\n",
      "문서:Document2 / TF:0.3010299956639812 / TF-IDF:0.09061905828945654\n",
      "문서:Document1 / TF:0.3010299956639812 / TF-IDF:0.09061905828945654\n",
      "is\n",
      "단어:is / IDF:0.3010299956639812\n",
      "문서:Document3 / TF:0.3010299956639812 / TF-IDF:0.09061905828945654\n",
      "문서:Document2 / TF:0.3010299956639812 / TF-IDF:0.09061905828945654\n",
      "문서:Document1 / TF:0.3010299956639812 / TF-IDF:0.09061905828945654\n",
      "a\n",
      "단어:a / IDF:0.47712125471966244\n",
      "문서:Document4 / TF:0.47712125471966244 / TF-IDF:0.227644691705265\n",
      "문서:Document1 / TF:0.47712125471966244 / TF-IDF:0.227644691705265\n",
      "sample\n",
      "단어:sample / IDF:0.07918124604762482\n",
      "문서:Document6 / TF:0.07918124604762482 / TF-IDF:0.006269669725654501\n",
      "문서:Document4 / TF:0.07918124604762482 / TF-IDF:0.006269669725654501\n",
      "문서:Document3 / TF:0.07918124604762482 / TF-IDF:0.006269669725654501\n",
      "문서:Document2 / TF:0.07918124604762482 / TF-IDF:0.006269669725654501\n",
      "문서:Document1 / TF:0.07918124604762482 / TF-IDF:0.006269669725654501\n",
      "another\n",
      "단어:another / IDF:0.7781512503836436\n",
      "문서:Document2 / TF:0.7781512503836436 / TF-IDF:0.6055193684736281\n",
      "not\n",
      "단어:not / IDF:0.17609125905568124\n",
      "문서:Document6 / TF:0.17609125905568124 / TF-IDF:0.031008131515815038\n",
      "문서:Document5 / TF:0.17609125905568124 / TF-IDF:0.031008131515815038\n",
      "문서:Document4 / TF:0.17609125905568124 / TF-IDF:0.031008131515815038\n",
      "문서:Document3 / TF:0.17609125905568124 / TF-IDF:0.031008131515815038\n"
     ]
    }
   ],
   "source": [
    "N = len(globalDocument)\n",
    "idfList = dict()\n",
    "documentLength = dict()\n",
    "\n",
    "for term, ptr in globalLexicon.items():\n",
    "    \n",
    "    df = 0\n",
    "    oldPtr = ptr\n",
    "    \n",
    "    print(term)\n",
    "    \n",
    "    while True:\n",
    "        if ptr == -1:\n",
    "            break\n",
    "            \n",
    "        df += 1\n",
    "        \n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "    ptr = oldPtr\n",
    "    idf = basicIDF(N, df)\n",
    "    idfList[term] = idf\n",
    "    \n",
    "    print(\"단어:{0} / IDF:{1}\".format(term, idf))\n",
    "\n",
    "    while True:\n",
    "        if ptr == -1:\n",
    "            break\n",
    "                  \n",
    "        postingData = globalPosting[ptr]\n",
    "        tf = postingData[2]\n",
    "        postingData[2] = tf * idf\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "        print(\"문서:{0} / TF:{1} / TF-IDF:{2}\".format(\n",
    "            globalDocument[postingData[1]],\n",
    "            tf,\n",
    "            postingData[2]\n",
    "        ))        \n",
    "        # { 색인어Idx, 문서Idx, TF, Next}\n",
    "        \n",
    "        if postingData[1] in documentLength.keys():\n",
    "            documentLength[postingData[1]] += postingData[2] ** 2\n",
    "        else:\n",
    "            documentLength[postingData[1]] = postingData[2] ** 2\n",
    "    \n",
    "    idf = basicIDF(N, df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(x, y):\n",
    "    return (x-y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"not\"\n",
    "\n",
    "queryPosting = dict()\n",
    "\n",
    "localPosting = dict()\n",
    "    \n",
    "for token in query.lower().split():\n",
    "    # Local에 대해서, 없으면 추가\n",
    "    if token not in queryPosting.keys():\n",
    "        queryPosting[token] =1\n",
    "    # 있으면, 빈도 증가\n",
    "    else:\n",
    "        queryPosting[token] += 1\n",
    "\n",
    "maxFreq = max(queryPosting.values())\n",
    "\n",
    "# Global Merge\n",
    "for token, freq in queryPosting.items(): \n",
    "    queryPosting[token] =  doubleNormalTF(0.5, freq, maxFreq)  * idfList[token]  # [0.5 -1] 쿼리인경우는 K 값을,  문서는"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not': 0.17609125905568124}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateList = dict()\n",
    "\n",
    "for term, ptr in globalLexicon.items():\n",
    "    queryWeight = 0\n",
    "    \n",
    "    if term in queryPosting.keys():\n",
    "        queryWeight = queryPosting[term]\n",
    "        \n",
    "    while True:\n",
    "        if ptr == -1:\n",
    "            break\n",
    "\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "        if postingData[1] in candidateList.keys():\n",
    "            candidateList[postingData[1]] += euclidean(queryWeight, postingData[2])\n",
    "        else:\n",
    "            candidateList[postingData[1]] = euclidean(queryWeight, postingData[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query\n",
      "순위:1 / 문서:Document5 / 거리:0.02104911389674908\n",
      "not\n",
      "순위:2 / 문서:Document6 / 거리:0.02108842265521787\n",
      "not sample\n",
      "순위:3 / 문서:Document3 / 거리:0.030044383207118004\n",
      "This is not sample\n",
      "순위:4 / 문서:Document1 / 거리:0.06828504187058979\n",
      "This is a sample\n",
      "순위:5 / 문서:Document4 / 거리:0.07291052831680302\n",
      "a not sample\n",
      "순위:6 / 문서:Document2 / 거리:0.38311664180570604\n",
      "This is another sample\n"
     ]
    }
   ],
   "source": [
    "resultList = sorted(candidateList.items(), key=lambda x:x[1])\n",
    "\n",
    "print(\"Query\")\n",
    "\n",
    "for i, (fileIdx, distance) in enumerate(resultList):\n",
    "    print(\"순위:{0} / 문서:{1} / 거리:{2}\".format(\n",
    "        (i+1), globalDocument[fileIdx], distance\n",
    "    ))\n",
    "    print(collection[fileIdx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.009956773530471006,\n",
       " 1: 0.38311664180570604,\n",
       " 0: 0.06828504187058979,\n",
       " 3: 0.05282291864015602,\n",
       " 5: 0.0010008129785708704,\n",
       " 4: 0.0009615042201020818}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innerProduct(x, y):\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateList = dict()\n",
    "\n",
    "# for term, ptr in globalLexicon.items():\n",
    "for term, weight in queryPosting.items():\n",
    "    \n",
    "    ptr = globalLexicon[term]\n",
    "\n",
    "    while True:\n",
    "        if ptr == -1:\n",
    "            break\n",
    "\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "        if postingData[1] in candidateList.keys():\n",
    "            candidateList[postingData[1]] += innerProduct(weight, postingData[2])\n",
    "        else:\n",
    "            candidateList[postingData[1]] = innerProduct(weight, postingData[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.00546026091958402,\n",
       " 4: 0.00546026091958402,\n",
       " 3: 0.00546026091958402,\n",
       " 2: 0.00546026091958402}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileIdx, sumProduct in candidateList.items():\n",
    "    candidateList[fileIdx] = sumProduct / documentLength[fileIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query\n",
      "순위:1 / 문서:Document5 / 거리:5.6788735872675735\n",
      "not\n",
      "순위:2 / 문서:Document6 / 거리:5.45582545040643\n",
      "not sample\n",
      "순위:3 / 문서:Document3 / 거리:0.5483966169235268\n",
      "This is not sample\n",
      "순위:4 / 문서:Document4 / 거리:0.10336916361590678\n",
      "a not sample\n"
     ]
    }
   ],
   "source": [
    "resultList = sorted(candidateList.items(), key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print(\"Query: \", query)\n",
    "\n",
    "for i, (fileIdx, distance) in enumerate(resultList):\n",
    "    print(\"순위:{0} / 문서:{1} / 거리:{2}\".format(\n",
    "        (i+1), globalDocument[fileIdx], distance\n",
    "    ))\n",
    "    print(collection[fileIdx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "ma = Kkma().morphs\n",
    "\n",
    "def getUniqueTermsBySet():\n",
    "    lexicon = list() # Unique term List\n",
    "\n",
    "    # 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "    for fileName in kobill.fileids():\n",
    "        document = kobill.open(fileName).read()\n",
    "\n",
    "        for token in document.split():\n",
    "            lexicon.extend(ma(token))\n",
    "            \n",
    "    return list(set(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = getUniqueTermsBySet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1487"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def docRepresentationByDefaultDict():\n",
    "    docList = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "#     defaultdict{\n",
    "#         문서번호(제목):{\n",
    "#             단어:1\n",
    "#         }\n",
    "        \n",
    "#     }\n",
    "#     collections = list()\n",
    "\n",
    "\n",
    "    # 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "    for fileName in kobill.fileids():\n",
    "#         collections.append(document)\n",
    "#         collections.index(document)\n",
    "        document = kobill.open(fileName).read()\n",
    "#         docVector = list(0 for _ in range(len(lexicon))) # 2638 차원을 갖는 백터\n",
    "#         docVector = defaultdict(int) # 공간을 차지해서..dict로 변경\n",
    "\n",
    "        for token in document.split():\n",
    "            for morpheme in ma(token):\n",
    "                docList[fileName][morpheme] += 1  # 속도 개선을 위해  index 체크하지 않음\n",
    "            \n",
    "    return docList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM = docRepresentationByDefaultDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invertedDocument(역문헌구조, 어휘)\n",
    "\n",
    "# DTM → TDM\n",
    "#    d1, d2, d3 ...... d10\n",
    "# t1  1,  0,   1           → Sparse → Dict\n",
    "# t2\n",
    "# t3\n",
    "\n",
    "# Numpy의 transpose를 쓰지 않는 이유 : 데이터 량 (1000개, 2000개 아님  몇십만개, 몇천만개)\n",
    "\n",
    "def invertedDocument(DTM):\n",
    "    TDM = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    # Only Python → Dictionary | Posting DB\n",
    "    # Dictionary → term, fp (Hash in memory)\n",
    "    # Posting → struct(docid, freq, next=fp) (FileDB)\n",
    "    for docName, docVector in DTM.items():\n",
    "        maxFreq = max(docVector.values())\n",
    "        for term, freq in docVector.items():\n",
    "            TDM[term][docName] = doubleNormalTF(0, freq, maxFreq)\n",
    "            \n",
    "    return TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM = invertedDocument(DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(DTM)\n",
    "\n",
    "def invertedWeight(TDM):\n",
    "    TWM = defaultdict(lambda:defaultdict(float))\n",
    "    # Document Vector Length\n",
    "    DVL = defaultdict(float)\n",
    "#     단어:{문서:TF, ....}\n",
    "    for term, tfList in TDM.items():\n",
    "        df = len(tfList)\n",
    "        idf = basicIDF(N, df)\n",
    "        \n",
    "        for docName, tf in tfList.items():\n",
    "            TWM[term][docName] = tf * idf\n",
    "            DVL[docName] += TWM[term][docName] ** 2\n",
    "    \n",
    "    return TWM, DVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWM, DVL = invertedWeight(TDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"국방의 의무와 보편적 교육에 대한 법안 법안\"\n",
    "\n",
    "queryRepresentation = defaultdict(float)\n",
    "\n",
    "for token in query.split():\n",
    "    for morpheme in ma(token):\n",
    "        queryRepresentation[morpheme] += 1 \n",
    "        \n",
    "maxFreq = max(queryRepresentation.values())        \n",
    "        \n",
    "for term, freq in queryRepresentation.items():\n",
    "\n",
    "    df = len(TWM[term])    \n",
    "    \n",
    "    if df > 0:\n",
    "        idf = basicIDF(N, df)\n",
    "        queryRepresentation[term] = doubleNormalTF(0.5, freq, maxFreq) * idf\n",
    "    else:\n",
    "        queryRepresentation[term] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'국방': 0.5242275032520142,\n",
       "             '의': 0.0,\n",
       "             '의무': 0.5242275032520142,\n",
       "             '와': 0.03431811792050636,\n",
       "             '보편적': 0,\n",
       "             '교육': 0.07268250975604232,\n",
       "             '에': 0.0,\n",
       "             '대하': 0.0,\n",
       "             'ㄴ': 0.0,\n",
       "             '법안': 0.3979400086720376})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryRepresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateList = defaultdict(float)\n",
    "\n",
    "for term, weight in queryRepresentation.items():\n",
    "    for docName, docWeight  in TWM[term].items():\n",
    "        candidateList[docName] += weight * docWeight\n",
    "        \n",
    "for docName, similarity in candidateList.items():\n",
    "    candidateList[docName] = similarity /DVL[docName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  국방의 의무와 보편적 교육에 대한 법안 법안\n",
      "순위:1 / 문서:1809892.txt / 유사도:0.02336253798221921\n",
      "['교육공무원법 일부개정법률안\\n\\n(정의화의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9892\\n\\n발의연월일 : 2010.', '11.', '12.', '발  의  자 : 정의화․이명수․김을동 \\n\\n이사철․여상규․안규백\\n\\n황영철․박영아․김정훈\\n\\n김학송 의원(10인)\\n\\n제안이유 및 주요내용\\n\\n  현행 교육공무원의 육아휴직은 만 6세 이하의 초등학교 취학 전 자\\n\\n녀의 육아를 위한 경우로 한정되어 있어 초등학교 취학 후 등하교 및 \\n\\n방과 후 양육 등에 어려움이 많고, 저학년 자녀 혼자 등하교를 하거나 \\n\\n어른 없는 집에서 지내다가 성폭력 범죄 등 흉악범죄의 피해자가 되\\n\\n고 있음.', '이에 육아휴직 가능 시기를 만 8세 이하의 자녀로 확대하여 자녀가 \\n\\n초등학교에 입학하여 학교생활에 순조롭게 적응할 수 있는 나이까지 \\n\\n교육공무원이 자녀양육을 위해서 휴직할 수 있도록 하려는 것임(안 제\\n\\n44조제1항제7호).']\n",
      "\n",
      "순위:2 / 문서:1809890.txt / 유사도:0.0174873431158863\n",
      "['지방공무원법 일부개정법률안\\n\\n(정의화의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9890\\n\\n발의연월일 : 2010.', '11.', '12.', '발  의  자 : 정의화․이명수․김을동 \\n\\n이사철․여상규․안규백\\n\\n황영철․박영아․김정훈\\n\\n김학송 의원(10인)\\n\\n제안이유 및 주요내용\\n\\n  초등학교 저학년의 경우에도 부모의 따뜻한 사랑과 보살핌이 필요\\n\\n한 나이이나, 현재 공무원이 자녀를 양육하기 위하여 육아휴직을 할 \\n\\n수 있는 자녀의 나이는 만 6세 이하로 되어 있어 초등학교 저학년인 \\n\\n자녀를 돌보기 위해서는 해당 부모님은 일자리를 그만 두어야 하고 \\n\\n이는 곧 출산의욕을 저하시키는 문제로 이어질 수 있을 것임.', '따라서 육아휴직이 가능한 자녀의 연령을 만 8세 이하로 개정하려\\n\\n는 것임(안 제63조제2항제4호).']\n",
      "\n",
      "순위:3 / 문서:1809891.txt / 유사도:0.01708218474498818\n",
      "['국가공무원법 일부개정법률안\\n\\n(정의화의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9891\\n\\n발의연월일 : 2010.', '11.', '12.', '발  의  자 : 정의화․이명수․김을동 \\n\\n이사철․여상규․안규백\\n\\n황영철․박영아․김정훈\\n\\n김학송 의원(10인)\\n\\n제안이유 및 주요내용\\n\\n  초등학교 저학년의 경우에도 부모의 따뜻한 사랑과 보살핌이 필요\\n\\n한 나이이나, 현재 공무원이 자녀를 양육하기 위하여 육아휴직을 할 \\n\\n수 있는 자녀의 나이는 만 6세 이하로 되어 있어 초등학교 저학년인 \\n\\n자녀를 돌보기 위해서는 해당 부모님은 일자리를 그만 두어야 하고 \\n\\n이는 곧 출산의욕을 저하시키는 문제로 이어질 수 있을 것임.', '따라서 육아휴직이 가능한 자녀의 연령을 만 8세 이하로 개정하려\\n\\n는 것임(안 제71조제2항제4호).']\n",
      "\n",
      "순위:4 / 문서:1809897.txt / 유사도:0.015425624222146153\n",
      "['국군부대의 아랍에미리트(UAE)군 교육훈련 지원 등에 \\n관한 파견 동의안\\n\\n의안\\n                                                                  제출연월일 :  2010.', '11.', '15.', '9897\\n번호\\n                                                                        제  출  자 :  정         부\\n\\n제안이유\\n\\n    가.  UAE측 요청과 협의에 따라,  국익창출과  다양한 지역에서의 \\n우리 특전부대 임무수행능력 향상 등을 목적으로  국군부대를 \\nUAE에 파견하려는 것임.', '나.']\n",
      "\n",
      "순위:5 / 문서:1809893.txt / 유사도:0.013962380668157791\n",
      "['남녀고용평등과 일 ·가정 양립 지원에 관한 법률 \\n\\n일부개정법률안\\n\\n(정의화의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9893\\n\\n발의연월일 : 2010.', '11.', '12.', '발  의  자 : 정의화․이명수․김을동 \\n\\n이사철․여상규․안규백\\n\\n황영철․박영아․김정훈\\n\\n김학송 의원(10인)\\n\\n제안이유 및 주요내용\\n\\n  현행법상 근로자가 육아휴직을 신청할 수 있는 경우는 만 6세 이하\\n\\n의 초등학교 취학 전 자녀를 양육하기 위한 경우임.', '그런데 초등학교 1․2학년의 경우 이들을 돌보는데 세심한 주의가 \\n\\n필요함에도 불구하고 사회에서의 돌보는 제도가 부족하여 아동대상 \\n\\n성폭력 등 범죄에 노출되어있고, 이로 인해 여성 근로자들은 직장생활\\n\\n을 그만두고 있는 실정임.']\n",
      "\n",
      "순위:6 / 문서:1809896.txt / 유사도:0.013853300683839354\n",
      "['행정절차법 일부개정법률안\\n\\n(유선호의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9896\\n\\n발의연월일 : 2010.', '11.', '15.', '발  의  자 : 유선호․강기갑․김효석  \\n\\n최문순ㆍ최재성ㆍ조영택  \\n\\n김성곤ㆍ문학진ㆍ백재현  \\n\\n송민순ㆍ양승조ㆍ신낙균  \\n\\n조배숙ㆍ박은수ㆍ정동영  \\n\\n김춘진ㆍ김재윤ㆍ우윤근  \\n\\n이성남ㆍ박영선 의원\\n\\n             (20인)\\n\\n제안이유\\n\\n  현행법은 입법예고와 행정예고를 통하여 정책 결정 과정에 국민 참\\n\\n여의 절차를 규정하고 있기는 하나 실제 정책 결정·집행·평가의 단계\\n\\n에서 근본적인 국민 참여 규정은 거의 없어 위임입법에 의하여 정책 \\n\\n결정 및 집행 권한이 부여되는 문제점이 있음.', '따라서 입법예고 이전의 국민적 협의절차와 재입법예고 규정 등을 \\n\\n신설하고, 당사자 등의 개념을 명확히 하여 당사자의 신청에 의한 청\\n\\n문의 기회를 보장하는 한편, 법령상의 일부 미비점을 개선․보완함으\\n\\n- 1 -\\n\\n\\x0c- 2 -\\n\\n로써 실질적인 국민 참여의 기회를 보장하여 행정에 대한 국민의 불\\n\\n신을 없애고 행정의 투명성을 확보하려는 것임.']\n",
      "\n",
      "순위:7 / 문서:1809899.txt / 유사도:0.01128430784198373\n",
      "['결혼중개업의 관리에 관한 법률 일부개정법률안\\n\\n(한선교의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9899\\n\\n발의연월일 : 2010.', '11.', '15.', '발  의  자 : 한선교․손범규․이인기 \\n\\n유성엽․이애주․이한성 \\n\\n안홍준․김태원․안형환 \\n\\n정갑윤 의원(10인)\\n\\n제안이유\\n\\n  최근 국제결혼의 상당수가 국제결혼중개업체를 통해 이루어지고 있\\n\\n으나, 일부 국제결혼중개업자가 이윤만을 추구하기 위하여 사실과 다\\n\\n른 정보나 거짓 정보를 제공하여 속성으로 성사된 국제결혼이 상대국\\n\\n과의 외교적 마찰이나 결혼생활의 조기 파탄을 야기하는 등 많은 문\\n\\n제가 발생하고 있음.', '이에 따라 건전한 국제결혼문화를 정착하고 결혼중개업에 의한 피해\\n\\n사례를 최소화하기 위해 국제결혼중개업의 등록기준을 강화하고 국제\\n\\n결혼중개업자의 인권침해적․불법적인 결혼중개행위에 대한 처벌규정을 \\n\\n보다 강화하고자 함.']\n",
      "\n",
      "순위:8 / 문서:1809898.txt / 유사도:0.009050790649016056\n",
      "['국군부대의 소말리아 해역 파견연장 동의안\\n\\n의안\\n                                                                  제출연월일 :  2010.', '11.', '15.', '9898\\n번호\\n                                                                        제  출  자 :  정         부\\n\\n제안이유\\n\\n      소말리아 아덴만 해역에 파견된 국군부대 ( 청해부대 )의 파견기간이 \\n2010년 12월 31일 종료될 예정이나,  다음 이유로 파견기간을 연장\\n하고자 함.', '첫째,  소말리아 해적활동으로 우리 선박의 안전이 위협을 받고 있고,\\n\\n      둘째,  청해부대가  성공적으로 임무를 수행하여 우리 국익보호 및  국위\\n선양에 기여하고 있으며,   \\n\\n      셋째,  국내외 관계기관에서 파견연장을 요청하고 있음.']\n",
      "\n",
      "순위:9 / 문서:1809894.txt / 유사도:0.0008885058243513874\n",
      "['고등교육법 일부개정법률안\\n\\n(안상수의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9894\\n\\n발의연월일 : 2010.', '11.', '15.', '발  의  자 : 안상수․김정훈․원희목 \\n\\n강석호․서상기․나성린 \\n\\n권영진․이춘식․정영희 \\n\\n이애주․안형환․백성운 \\n\\n김금래 의원(13인)\\n\\n제안이유 및 주요내용\\n\\n  현재 간호사의 경우 전문대학 졸업 또는 대학 졸업에 상관없이 면\\n\\n허증을 취득할 수 있지만, 학위의 종류가 전문학사이기 때문에 학사학\\n\\n위를 취득하기 위하여 87.2%가 별도로 학사학위 교육과정을 이수하고 \\n\\n있는 실정임.', '이러한 4년제 간호 교육의 필요성과 선진 각국의 경향을 고려하고 \\n\\n국민에 대한 보다 나은 의료 서비스를 제공하기 위하여 대통령령이 \\n\\n정하는 일정한 기준을 충족하는 간호과에 대해서는 수업연한을 4년으\\n\\n로 하고, 수여하는 학위의 종류를 학사학위로 하도록 함(안 제50조의3 \\n\\n신설).']\n",
      "\n",
      "순위:10 / 문서:1809895.txt / 유사도:0.0003082639264713437\n",
      "['하도급거래 공정화에 관한 법률 일부개정법률안\\n\\n(유선호의원 대표발의 )\\n\\n 의 안\\n 번 호\\n\\n9895\\n\\n발의연월일 : 2010.', '11.', '15.', '발  의  자 : 유선호․강기갑․김효석  \\n\\n조승수ㆍ최문순ㆍ조영택  \\n\\n문학진ㆍ백재현ㆍ송민순  \\n\\n박은수ㆍ정동영ㆍ김춘진  \\n\\n김재윤ㆍ우윤근ㆍ이성남  \\n\\n이종걸 의원(16인)\\n\\n제안이유 및 주요내용\\n\\n  원․수급사업자 사이의 하도급거래는 외형적으로 공정한 계약을 체\\n\\n결할지라도 교섭력에서 절대 우위에 있는 원사업자에 의한 불공정한 \\n\\n행위의 가능성은 여전히 상존하고 있음.', '또한 수급사업자는 원사업자\\n\\n의 부당행위에 의하여 손해를 입게 된 경우에도 입증의 부담을 안기 \\n\\n때문에 민사소송에 의한 피해구제도 쉽지 않은 실정임.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "resultList = sorted(candidateList.items(), key= lambda x:x[1], reverse=True)\n",
    "\n",
    "print(\"Query: \", query)\n",
    "\n",
    "for i, (docName, similarity) in enumerate(resultList):\n",
    "    print(\"순위:{0} / 문서:{1} / 유사도:{2}\".format(\n",
    "        (i+1), docName, similarity\n",
    "    ))\n",
    "    \n",
    "    content = kobill.open(docName).read()\n",
    "    print(sent_tokenize(content)[:5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
