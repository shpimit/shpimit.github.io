{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = list() # Unique term List\n",
    "\n",
    "# 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "for fileName in kobill.fileids():\n",
    "    document = kobill.open(fileName).read()\n",
    "    \n",
    "    for token in document.split():\n",
    "        if token not in lexicon:\n",
    "            lexicon.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2638, 2638)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon), len(set(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueTerms():\n",
    "    lexicon = list() # Unique term List\n",
    "\n",
    "    # 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "    for fileName in kobill.fileids():\n",
    "        document = kobill.open(fileName).read()\n",
    "\n",
    "        for token in document.split():\n",
    "            if token not in lexicon:\n",
    "                lexicon.append(token)\n",
    "        \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueTermsBySet():\n",
    "    lexicon = list() # Unique term List\n",
    "\n",
    "    # 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "    for fileName in kobill.fileids():\n",
    "        document = kobill.open(fileName).read()\n",
    "\n",
    "        for token in document.split():\n",
    "            lexicon.append(token) \n",
    "            \n",
    "    return list(set(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 ms ± 1.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getUniqueTerms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46 ms ± 29.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit getUniqueTermsBySet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = getUniqueTermsBySet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2638, 2638)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon), len(set(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.index(\"외교부,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docRepresentation(lexicon):\n",
    "    docList = list() \n",
    "    # 문서를 표현 (사전을 이용해서)\n",
    "    # 사전의 전체 갯수(Unique 단어 수 : N) → N차원\n",
    "    # 단어가 있으면 1, 없으면 0\n",
    "    # 사전에 위치한 단어의 index\n",
    "    # 1개의 문서에 대한 벡터 표현\n",
    "    # list(list()) → 2차원\n",
    "    # Bag Of Words\n",
    "    # Document-Term Matrix : DTM\n",
    "    #    w1, w2, w3 .....wn\n",
    "    # d1  1,  0,  1 .....   docVector\n",
    "    # d2\n",
    "    # d3\n",
    "    # ...\n",
    "    # d10 → docList\n",
    "\n",
    "    # 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "    for fileName in kobill.fileids():\n",
    "        document = kobill.open(fileName).read()\n",
    "        docVector = list(0 for _ in range(len(lexicon))) # 2638 차원을 갖는 백터\n",
    "\n",
    "        for token in document.split():\n",
    "            docVector[lexicon.index(token)] = 1\n",
    "        docList.append(docVector) \n",
    "            \n",
    "    return docList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docList = docRepresentation(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docList), #lexicon[1], docList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docRepresentationByDict(lexicon):\n",
    "    docList = list() \n",
    "    # 문서를 표현 (사전을 이용해서)\n",
    "    # 사전의 전체 갯수(Unique 단어 수 : N) → N차원\n",
    "    # 단어가 있으면 1, 없으면 0\n",
    "    # 사전에 위치한 단어의 index\n",
    "    # 1개의 문서에 대한 벡터 표현\n",
    "    # list(list()) → 2차원\n",
    "    # Bag Of Words\n",
    "    #    w1, w2, w3 .....wn\n",
    "    # d1  1,  0,  1 .....   docVector\n",
    "    # d2\n",
    "    # d3\n",
    "    # ...\n",
    "    # d10 → docList\n",
    "\n",
    "    # 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "    for fileName in kobill.fileids():\n",
    "        document = kobill.open(fileName).read()\n",
    "#         docVector = list(0 for _ in range(len(lexicon))) # 2638 차원을 갖는 백터\n",
    "        docVector = dict() # 공간을 차지해서..dict로 변경\n",
    "\n",
    "        for token in document.split():\n",
    "            docVector[lexicon.index(token)] = 1\n",
    "        docList.append(docVector) \n",
    "            \n",
    "    return docList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docList = docRepresentationByDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docList), #exicon[1], docList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 ms ± 2.75 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  docRepresentation(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 ms ± 2.96 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit docRepresentationByDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def docRepresentationByDefaultDict(lexicon):\n",
    "    docList = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "#     defaultdict{\n",
    "#         문서번호(제목):{\n",
    "#             단어:1\n",
    "#         }\n",
    "        \n",
    "#     }\n",
    "#     collections = list()\n",
    "\n",
    "\n",
    "    # 사전(Lexicon) 만들기 → 우리(Collection = Crawled data) 세상(N차원)\n",
    "    for fileName in kobill.fileids():\n",
    "#         collections.append(document)\n",
    "#         collections.index(document)\n",
    "        document = kobill.open(fileName).read()\n",
    "#         docVector = list(0 for _ in range(len(lexicon))) # 2638 차원을 갖는 백터\n",
    "#         docVector = defaultdict(int) # 공간을 차지해서..dict로 변경\n",
    "\n",
    "        for token in document.split():\n",
    "            docList[fileName][token] = 1  # 속도 개선을 위해  index 체크하지 않음\n",
    "            \n",
    "    return docList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docList = docRepresentationByDefaultDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12 ms ± 39.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit docRepresentationByDefaultDict(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1809897.txt', '1809898.txt'], ['1809896.txt']]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Search Algorithm(Retrieval Model) = Boolean Model\n",
    "\n",
    "query = [\"국회\", \"의원\"]\n",
    "\n",
    "result = list()\n",
    "for token in query:                                  # |Q| = q1, q2       → O(q*d*v)\n",
    "    candidate = list()\n",
    "    for docName, docVector in docList.items():       # |D| = \n",
    "        for term, freq in docVector.items():\n",
    "            if term == token:\n",
    "                candidate.append(docName)\n",
    "                \n",
    "    result.append(candidate)\n",
    "\n",
    "print(result)\n",
    "# Search Algorithm(Retrieval Model) = Boolean Model\n",
    "# &, |, not\n",
    "# Stack → 4칙 연산계산기(+, -)\n",
    "print(list(set(result[0]).intersection(result[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Space Model 유사도 검색(유클리디안 distance)\n",
    "result = list()\n",
    "for token in query:\n",
    "    candidate = list()\n",
    "    for docName, docVector in docList.items():\n",
    "        for term, freq in docVector.items():\n",
    "            if term == token:\n",
    "                candidate.append(docName)\n",
    "                \n",
    "    result.append(candidate)\n",
    "\n",
    "print(result)\n",
    "# Search Algorithm(Retrieval Model) = Boolean Model\n",
    "# &, |\n",
    "# Stack → 4칙 연산계산기(+, -)\n",
    "print(list(set(result[0]).intersection(result[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invertedDocument(역문헌구조, 어휘)\n",
    "\n",
    "# DTM → TDM\n",
    "#    d1, d2, d3 ...... d10\n",
    "# t1  1,  0,   1           → Sparse → Dict\n",
    "# t2\n",
    "# t3\n",
    "\n",
    "# Numpy의 transpose를 쓰지 않는 이유 : 데이터 량 (1000개, 2000개 아님  몇십만개, 몇천만개)\n",
    "\n",
    "def invertedDocument(DTM):\n",
    "    TDM = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Only Python → Dictionary | Posting DB\n",
    "    # Dictionary → term, fp (Hash in memory)\n",
    "    # Posting → struct(docid, freq, next=fp) (FileDB)\n",
    "    for docName, docVector in DTM.items():\n",
    "        for term, freq in docVector.items():\n",
    "            TDM[term][docName] = freq\n",
    "            \n",
    "    return TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM = invertedDocument(docList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int, {'1809897.txt': 1, '1809898.txt': 1}),\n",
       " defaultdict(int, {'1809896.txt': 1}))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDM[\"국회\"], TDM[\"의원\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrievalByBoolean():\n",
    "    \n",
    "    result = list()\n",
    "    for token in query:                                  # |Q| = q1, q2       → O(q*d*v)\n",
    "        candidate = list()\n",
    "        for docName, docVector in docList.items():       # |D| = \n",
    "            for term, freq in docVector.items():\n",
    "                if term == token:\n",
    "                    candidate.append(docName)\n",
    "\n",
    "        result.append(candidate)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrievalByTDM():\n",
    "    result = list()\n",
    "    for token in query:\n",
    "        result.append(TDM[token])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463 µs ± 10.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit retrievalByBoolean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 ns ± 3.59 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit retrievalByTDM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
