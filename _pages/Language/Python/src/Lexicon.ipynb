{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 :  오늘 미세먼지는 어제 미세먼지보다 나빠요\n"
     ]
    }
   ],
   "source": [
    "sentence = \"오늘 미세먼지는 어제 미세먼지보다 나빠요\"\n",
    "\n",
    "print(\"원본 : \", sentence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '미세', '먼지', '미세', '먼지']\n",
      "['오늘', '미세', '미세먼지', '먼지']\n"
     ]
    }
   ],
   "source": [
    "ma = Kkma()\n",
    "\n",
    "print([token[0] for token in ma.pos(sentence) if token[1].startswith(\"NN\")])\n",
    "\n",
    "print(ma.nouns(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramUmjeol(sentence, n=2):\n",
    "    result = []\n",
    "    tokens = list(sentence)\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "#         result.append(tokens[i:i+n])\n",
    "#         result.append(tuple(tokens[i:i+n]))\n",
    "        result.append(\"\".join(tokens[i:i+n]))\n",
    "        \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramEojeol(sentence, n=2):\n",
    "    result = []\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    for i in range(len(tokens)-n+1):\n",
    "#         result.append(tokens[i:i+n])\n",
    "#         result.append(tuple(tokens[i:i+n]))\n",
    "        result.append(\"\".join(tokens[i:i+n]))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어절 단위 분리:  ['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요']\n",
      "형태소 분석:  ['미세먼지보다', '어제', '아요', '나빠요', '미세', '보다', '미세먼지는', '먼지', '는', '나쁘', '오늘']\n",
      "명사분석:  ['미세먼지보다', '어제', '아요', '나빠요', '미세', '보다', '미세먼지는', '미세먼지', '먼지', '는', '나쁘', '오늘']\n",
      "어절(바이그램) 분석:  ['나빠요', '미세', '먼지는', '미세먼지는', '어제', '아요', '어제미세', '먼지', '보다나쁘', '나쁘', '미세먼지보다', '는어제', '먼지보다', '는', '오늘', '나쁘아요', '보다', '미세먼지', '오늘미세']\n"
     ]
    }
   ],
   "source": [
    "lexicon = list()\n",
    "\n",
    "th =1 \n",
    "# Tokenizing\n",
    "lexicon = [token for token in word_tokenize(sentence) if len(token) > th]\n",
    "print(\"어절 단위 분리: \", lexicon)\n",
    "\n",
    "# 품사\n",
    "for token in [token for token in word_tokenize(sentence) if len(token) > th]:\n",
    "    lexicon.extend([token[0] for token in ma.pos(token)])\n",
    "print(\"형태소 분석: \", list(set(lexicon)))\n",
    "\n",
    "# \n",
    "for token in [token for token in word_tokenize(sentence) if len(token) > th]:\n",
    "    lexicon.extend(ma.nouns(token))\n",
    "print(\"명사분석: \", list(set(lexicon)))\n",
    "\n",
    "lexicon.extend(ngramEojeol(\" \".join([token[0] for token in  ma.pos(sentence)])))\n",
    "print(\"어절(바이그램) 분석: \", list(set(lexicon)))\n",
    "\n",
    "\n",
    "newLexicon = list()\n",
    "for term in lexicon:\n",
    "    lexicon.extend(ngramUmjeol(term))\n",
    "lexicon.extend([term for term in newLexicon if len(term.strip()) > th])    \n",
    "print(\"음절(바이그램) 분석: \", list(set(lexicon)))\n",
    "\n",
    "    \n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "path = \"../data\"\n",
    "listdir(path)\n",
    "\n",
    "# fileids() → 말뭉치 목록을 리턴\n",
    "\n",
    "def getFileList(base=\"./\", ext=\"txt\"):\n",
    "    fileList = list()\n",
    "    \n",
    "    for file in listdir(base):\n",
    "        if file.split(\".\")[-1] == ext:\n",
    "            fileList.append(\"{0}/{1}\".format(base, file))\n",
    "            \n",
    "            \n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(getFileList(\"../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354316"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getContent(getFileList(\"../data\")[0])\n",
    "len(getContent(getFileList(\"../data\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = getContent(getFileList(\"../data\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "print(punctuation)\n",
    "\n",
    "# pattern = re.compile(r\"[\\/\\(\\)\\]{\\}{2,}\")\n",
    "pattern = re.compile(r\"[%s]{2,}\" % re.escape(punctuation))\n",
    "pattern.findall(content)\n",
    "pattern.sub(\" \", content)\n",
    "\n",
    "pattern = re.compile(r\"[A-Za-z]{8,}\")\n",
    "pattern.findall(content)\n",
    "\n",
    "# email #email.com\n",
    "pattern = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{3,}@[A-Za-z0-9\\-\\_\\.]{3,}(.[A-Za-z0-9\\-\\_\\.]{2,})+)\")\n",
    "pattern.findall(content)\n",
    "pattern.sub(\" \", content)\n",
    "\n",
    "# www .domain .com\n",
    "# www .domain .co .kr\n",
    "# m.domain\n",
    "pattern = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{1,}(.[A-Za-z0-9\\-\\_\\.]{2,})+)\")\n",
    "pattern.findall(content)\n",
    "content = pattern.sub(\" \", content)\n",
    "\n",
    "pattern = re.compile(r\"\\s{2,}\")\n",
    "pattern.findall(content)\n",
    "content = pattern.sub(\" \", content)\n",
    "\n",
    "pattern = re.compile(r\"([^ㄱ-ㅎ ㅏ_|가-힣0-9]+)\")\n",
    "pattern.findall(content)\n",
    "content = pattern.sub(\" \", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  getPatternList():\n",
    "    patternList = dict()\n",
    "    \n",
    "    pattern = re.compile(r\"[%s]{2,}\" % re.escape(punctuation))\n",
    "    patternList['Punctuation'] = pattern\n",
    "    \n",
    "    pattern = re.compile(r\"[A-Za-z]{8,}\")\n",
    "    patternList['Nonword'] = pattern\n",
    "    \n",
    "    pattern = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{3,}@[A-Za-z0-9\\-\\_\\.]{3,}(.[A-Za-z0-9\\-\\_\\.]{2,})+)\")\n",
    "    patternList['Email'] = pattern\n",
    "    \n",
    "    \n",
    "    pattern = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{1,}(.[A-Za-z0-9\\-\\_\\.]{2,})+)\")    \n",
    "    patternList['Domain'] = pattern\n",
    "    \n",
    "    pattern = re.compile(r\"\\s{2,}\")    \n",
    "    patternList['Whitespace'] = pattern\n",
    "    \n",
    "    pattern = re.compile(r\"([^ㄱ-ㅎ ㅏ_|가-힣0-9]+)\")\n",
    "    patternList['Korean'] = pattern\n",
    "\n",
    "    return patternList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternList = getPatternList()\n",
    "\n",
    "content = getContent(getFileList(\"../data\")[0])\n",
    "\n",
    "for _ in [\"Email\", \"Domain\", \"Nonword\", \"Punctuation\", \"Whitespace\"]:\n",
    "    content = patternList[_].sub(\" \", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "termList = list() # 지난 시간의  Lexicon\n",
    "posList = list()\n",
    "nounLIst = list()\n",
    "ngramList = list()\n",
    "\n",
    "th = 1\n",
    "\n",
    "for sentence in sent_tokenize(content):\n",
    "    for token in word_tokenize(sentence):\n",
    "        if len(token) > th:\n",
    "            termList.append(token)\n",
    "            posList.append([morpheme for morpheme in ma.morphs(token) if len(morpheme) > th])\n",
    "            nounLIst.extend([noun for noun in ma.nouns(token) if len(noun) > th])\n",
    "            ngramList.extend(ngramUmjeol(token))\n",
    "\n",
    "termLIst = list(set(termList))\n",
    "posList = list(set(posList))\n",
    "nounLIst = list(set(nounList))\n",
    "ngramList = list(set(ngramList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(termList), len(posList), len(nounList), len(ngramList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
