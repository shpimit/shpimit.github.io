{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day_01_04_mnist.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import  input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_basic():\n",
    "    np.set_printoptions(linewidth=1000)\n",
    "\n",
    "    # 0~1사이의 mnist 값을 가지고 있는것,  다른 mnist데이터는 다르게 들어갈수 있다.\n",
    "    mnist = input_data.read_data_sets('mnist')\n",
    "\n",
    "    print(mnist.train.images.shape)       # (55000, 784)\n",
    "    print(mnist.validation.images.shape)  # ( 5000, 784)\n",
    "    print(mnist.test.images.shape)        # (10000, 784)\n",
    "\n",
    "    print(mnist.train.labels.shape)       # (55000, )\n",
    "    print(mnist.train.labels[:5])         # [7 3 4 6 1]\n",
    "    # print(mnist.train.images[0])\n",
    "    # pixel 기준으로 보게\n",
    "    # print((mnist.train.images[0].reshape(28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocastic Gradient Decent(SGD)\n",
    "* 아래 소스는 weight를 다 돌고 나서 한번에 한다. 안좋은 방법."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_sgd():\n",
    "    mnist = input_data.read_data_sets('mnist')\n",
    "\n",
    "    # 784가 feature, 28*28 중에 1개의 pixel이 1의 feature가 된다.\n",
    "    # w = tf.Variable(tf.random_uniform([784, 10]))\n",
    "    # 모델 정확도 높이기 위해 아님  ->  빨리 수렴한다.\n",
    "    # get_variable은 변수가 만들어져 있으면 가져오고 , 없으면 만든다.  glorot은  xavier\n",
    "    w = tf.get_variable('w', shape=[784,10], initializer=tf.initializers.glorot_normal())\n",
    "    # w = tf.Variable(tf.contrib.layers.xavier_initializer([784,10]))\n",
    "    # bias는 class의 갯수만큼 정한다.\n",
    "    # b = tf.Variable(tf.random_uniform([10]))\n",
    "    # 정확도 올리기위해\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    ph_x = tf.placeholder(tf.float32)\n",
    "\n",
    "    # (55000, 10) = (55000, 784) @  (784, 10)\n",
    "    z = tf.matmul(ph_x, w) + b\n",
    "    hx = tf.nn.softmax(z)\n",
    "\n",
    "    # tensorflow 2.0d에서 정리 될것,  logit=예측  label을 결과괌\n",
    "    # logit에 z를 하는것은 hx를 전달하지 않아도 됨, 자체적으로 함\n",
    "    # 우리의 결과는 ont-hot 백터가 아님으로  sparse_softmax_cross_entropy_with_logits\n",
    "    loss_i = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=z, labels=np.int32(mnist.train.labels))\n",
    "    loss = tf.reduce_mean(loss_i)\n",
    "\n",
    "    # 모델 정확도 높이기 위해 아님  ->  빨리 수렴한다.\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train = optimizer.minimize(loss=loss)\n",
    "\n",
    "    # sess로 변수의 값을 알수 있다.\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(100):\n",
    "        sess.run(train, feed_dict={ph_x: mnist.train.images})\n",
    "        print(i, sess.run(loss, {ph_x: mnist.train.images}))\n",
    "\n",
    "    preds = sess.run(hx, {ph_x: mnist.test.images})\n",
    "    preds_arg = np.argmax(preds, axis=1)  # 1: 수평, 0: 수직\n",
    "    # spase일때는 argmax를 가져올필요 없다.\n",
    "    # test_arg  = np.argmax(mnist.test.labels, axis=1)\n",
    "\n",
    "    # 파이썬의 list는 broadcasting기능이 없어서, numpy array로 변경\n",
    "    # grades = np.array(['Setosa', 'Versicolor', 'Virginica'])\n",
    "    # print(grades[preds_arg])\n",
    "    # print(preds)\n",
    "\n",
    "    # 1차 혼돈 : 데이터가 섞여 있지 않음으로 인한오류 → shuffle 필요 np.random.shuffle(iris)\n",
    "    # 2차 혼돈 : 돌릴때 마다 위치가 달라져서 ...np.random.seed(1)\n",
    "    print('acc: ', np.mean(preds_arg == mnist.test.labels))\n",
    "\n",
    "    print(preds_arg)\n",
    "    # print(test_arg)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-78923763d920>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\shpim\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\shpim\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\shpim\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\shpim\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\shpim\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\shpim\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "0 1.962099\n",
      "1 1.6237156\n",
      "2 1.3404368\n",
      "3 1.1272396\n",
      "4 0.9743591\n",
      "5 0.85995346\n",
      "6 0.7724554\n",
      "7 0.7066946\n",
      "8 0.65712285\n",
      "9 0.6181779\n",
      "10 0.58579516\n",
      "11 0.557658\n",
      "12 0.5330468\n",
      "13 0.51204383\n",
      "14 0.4945204\n",
      "15 0.47987068\n",
      "16 0.46735656\n",
      "17 0.4564141\n",
      "18 0.4466758\n",
      "19 0.43787408\n",
      "20 0.42980608\n",
      "21 0.42236397\n",
      "22 0.41552272\n",
      "23 0.40926448\n",
      "24 0.40352464\n",
      "25 0.39821044\n",
      "26 0.3932488\n",
      "27 0.3886023\n",
      "28 0.3842514\n",
      "29 0.38017935\n",
      "30 0.3763757\n",
      "31 0.37283567\n",
      "32 0.3695406\n",
      "33 0.36644328\n",
      "34 0.3634831\n",
      "35 0.3606198\n",
      "36 0.35785085\n",
      "37 0.35519588\n",
      "38 0.35267055\n",
      "39 0.35027388\n",
      "40 0.34799495\n",
      "41 0.3458216\n",
      "42 0.3437405\n",
      "43 0.34173772\n",
      "44 0.33980444\n",
      "45 0.33794197\n",
      "46 0.33615747\n",
      "47 0.334453\n",
      "48 0.33282074\n",
      "49 0.33124822\n",
      "50 0.3297266\n",
      "51 0.3282517\n",
      "52 0.32682198\n",
      "53 0.32543725\n",
      "54 0.32409838\n",
      "55 0.32280555\n",
      "56 0.3215549\n",
      "57 0.32033998\n",
      "58 0.31915554\n",
      "59 0.31800097\n",
      "60 0.3168782\n",
      "61 0.31578863\n",
      "62 0.31473145\n",
      "63 0.31370446\n",
      "64 0.31270537\n",
      "65 0.3117316\n",
      "66 0.31078118\n",
      "67 0.30985326\n",
      "68 0.30894822\n",
      "69 0.3080661\n",
      "70 0.3072058\n",
      "71 0.30636555\n",
      "72 0.3055441\n",
      "73 0.3047405\n",
      "74 0.30395412\n",
      "75 0.30318466\n",
      "76 0.30243182\n",
      "77 0.30169496\n",
      "78 0.3009731\n",
      "79 0.30026466\n",
      "80 0.29956883\n",
      "81 0.29888558\n",
      "82 0.29821473\n",
      "83 0.2975562\n",
      "84 0.29690948\n",
      "85 0.29627424\n",
      "86 0.29564983\n",
      "87 0.2950359\n",
      "88 0.29443216\n",
      "89 0.29383856\n",
      "90 0.29325476\n",
      "91 0.29268032\n",
      "92 0.29211488\n",
      "93 0.29155806\n",
      "94 0.29100958\n",
      "95 0.2904695\n",
      "96 0.28993732\n",
      "97 0.28941312\n",
      "98 0.28889665\n",
      "99 0.28838766\n",
      "acc:  0.9219\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "mnist_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 1\n",
    "* 테스트셋의 정확도를 알려주세요.\n",
    "* hint : 데이터를 쪼개쓰는 mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_softmax_mini_batch():\n",
    "    mnist = input_data.read_data_sets('mnist')\n",
    "\n",
    "    # 784가 feature, 28*28 중에 1개의 pixel이 1의 feature가 된다.\n",
    "    # w = tf.Variable(tf.random_uniform([784, 10]))\n",
    "    # 모델 정확도 높이기 위해 아님  ->  빨리 수렴한다.\n",
    "    # get_variable은 변수가 만들어져 있으면 가져오고 , 없으면 만든다.  glorot은  xavier\n",
    "    w = tf.get_variable('w2', shape=[784,10], initializer=tf.initializers.glorot_normal())\n",
    "    # w = tf.Variable(tf.contrib.layers.xavier_initializer([784,10]))\n",
    "    # bias는 class의 갯수만큼 정한다.\n",
    "    # b = tf.Variable(tf.random_uniform([10]))\n",
    "    # 정확도 올리기위해\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    ph_x = tf.placeholder(tf.float32)\n",
    "    ph_y = tf.placeholder(tf.int32)\n",
    "\n",
    "    # (55000, 10) = (55000, 784) @  (784, 10)\n",
    "    z = tf.matmul(ph_x, w) + b\n",
    "    hx = tf.nn.softmax(z)\n",
    "\n",
    "    # tensorflow 2.0d에서 정리 될것,  logit=예측  label을 결과괌\n",
    "    # logit에 z를 하는것은 hx를 전달하지 않아도 됨, 자체적으로 함\n",
    "    # 우리의 결과는 ont-hot 백터가 아님으로  sparse_softmax_cross_entropy_with_logits\n",
    "    loss_i = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=z, labels=ph_y)\n",
    "    loss = tf.reduce_mean(loss_i)\n",
    "\n",
    "    # 모델 정확도 높이기 위해 아님  ->  빨리 수렴한다.\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train = optimizer.minimize(loss=loss)\n",
    "\n",
    "    # sess로 변수의 값을 알수 있다.\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 100   # 출구를 몇개씩 조사할건지\n",
    "    n_iters = mnist.train.num_examples // batch_size # 550\n",
    "\n",
    "    # epcochs를 1번 돌고 념\n",
    "    for i in range(epochs):\n",
    "        c = 0\n",
    "        for j in range(n_iters):\n",
    "            xx, yy = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            sess.run(train, feed_dict={ph_x: xx, ph_y: yy})\n",
    "            c += sess.run(loss, {ph_x: xx, ph_y:yy})\n",
    "\n",
    "        print(i, c/ n_iters)\n",
    "\n",
    "    preds = sess.run(hx, {ph_x: mnist.test.images})\n",
    "    preds_arg = np.argmax(preds, axis=1)  # 1: 수평, 0: 수직\n",
    "    # spase일때는 argmax를 가져올필요 없다.\n",
    "    # test_arg  = np.argmax(mnist.test.labels, axis=1)\n",
    "\n",
    "    # 파이썬의 list는 broadcasting기능이 없어서, numpy array로 변경\n",
    "    # grades = np.array(['Setosa', 'Versicolor', 'Virginica'])\n",
    "    # print(grades[preds_arg])\n",
    "    # print(preds)\n",
    "\n",
    "    # 1차 혼돈 : 데이터가 섞여 있지 않음으로 인한오류 → shuffle 필요 np.random.shuffle(iris)\n",
    "    # 2차 혼돈 : 돌릴때 마다 위치가 달라져서 ...np.random.seed(1)\n",
    "    print('acc: ', np.mean(preds_arg == mnist.test.labels))\n",
    "\n",
    "    print(preds_arg)\n",
    "    # print(test_arg)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0 0.34581849932670594\n",
      "1 0.28294095662507146\n",
      "2 0.27262380918318574\n",
      "3 0.2672042059221051\n",
      "4 0.26281767536293377\n",
      "5 0.26140581697225573\n",
      "6 0.2604996431144801\n",
      "7 0.25685677154497666\n",
      "8 0.25420204677365044\n",
      "9 0.25061307678845796\n",
      "acc:  0.9208\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "mnist_softmax_mini_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멀티 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_multi_layers():\n",
    "    mnist = input_data.read_data_sets('mnist')\n",
    "\n",
    "    w1 = tf.get_variable('w11', shape=[784, 512], initializer=tf.initializers.glorot_normal())\n",
    "    w2 = tf.get_variable('w22', shape=[512, 256], initializer=tf.initializers.glorot_normal())\n",
    "    w3 = tf.get_variable('w32', shape=[256,  10], initializer=tf.initializers.glorot_normal())\n",
    "    b1 = tf.Variable(tf.zeros([512]))\n",
    "    b2 = tf.Variable(tf.zeros([256]))\n",
    "    b3 = tf.Variable(tf.zeros([ 10]))\n",
    "\n",
    "    ph_x = tf.placeholder(tf.float32)\n",
    "    ph_y = tf.placeholder(tf.int32)\n",
    "\n",
    "    # (55000, 10) = (55000, 784) @  (784, 10)\n",
    "    z1 = tf.matmul(ph_x, w1) + b1\n",
    "    r1 = tf.nn.relu(z1)\n",
    "\n",
    "    z2 = tf.matmul(r1, w2) + b2\n",
    "    r2 = tf.nn.relu(z2)\n",
    "\n",
    "    z3 = tf.matmul(r2, w3) + b3\n",
    "    hx = tf.nn.softmax(z3)\n",
    "\n",
    "    # tensorflow 2.0d에서 정리 될것,  logit=예측  label을 결과괌\n",
    "    # logit에 z를 하는것은 hx를 전달하지 않아도 됨, 자체적으로 함\n",
    "    # 우리의 결과는 ont-hot 백터가 아님으로  sparse_softmax_cross_entropy_with_logits\n",
    "    loss_i = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=z3, labels=ph_y)\n",
    "    loss = tf.reduce_mean(loss_i)\n",
    "\n",
    "    # 모델 정확도 높이기 위해 아님  ->  빨리 수렴한다.\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train = optimizer.minimize(loss=loss)\n",
    "\n",
    "    # sess로 변수의 값을 알수 있다.\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 100   # 출구를 몇개씩 조사할건지\n",
    "    n_iters = mnist.train.num_examples // batch_size # 550\n",
    "\n",
    "    # epcochs를 1번 돌고 념\n",
    "    for i in range(epochs):\n",
    "        c = 0\n",
    "        for j in range(n_iters):\n",
    "            xx, yy = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            sess.run(train, feed_dict={ph_x: xx, ph_y: yy})\n",
    "            c += sess.run(loss, {ph_x: xx, ph_y:yy})\n",
    "\n",
    "        print(i, c/ n_iters)\n",
    "\n",
    "    preds = sess.run(hx, {ph_x: mnist.test.images})\n",
    "    preds_arg = np.argmax(preds, axis=1)  # 1: 수평, 0: 수직\n",
    "    # spase일때는 argmax를 가져올필요 없다.\n",
    "    # test_arg  = np.argmax(mnist.test.labels, axis=1)\n",
    "\n",
    "    # 파이썬의 list는 broadcasting기능이 없어서, numpy array로 변경\n",
    "    # grades = np.array(['Setosa', 'Versicolor', 'Virginica'])\n",
    "    # print(grades[preds_arg])\n",
    "    # print(preds)\n",
    "\n",
    "    # 1차 혼돈 : 데이터가 섞여 있지 않음으로 인한오류 → shuffle 필요 np.random.shuffle(iris)\n",
    "    # 2차 혼돈 : 돌릴때 마다 위치가 달라져서 ...np.random.seed(1)\n",
    "    print('acc: ', np.mean(preds_arg == mnist.test.labels))\n",
    "\n",
    "    print(preds_arg)\n",
    "    # print(test_arg)\n",
    "\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0 0.17192414816807616\n",
      "1 0.08467089972712777\n",
      "2 0.0689768023060804\n",
      "3 0.056971493860368025\n",
      "4 0.0562265362912281\n",
      "5 0.04805349267225459\n",
      "6 0.0424938825201984\n",
      "7 0.043625768014674327\n",
      "8 0.03891693604199893\n",
      "9 0.032483553080528506\n",
      "acc:  0.9728\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "mnist_multi_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
